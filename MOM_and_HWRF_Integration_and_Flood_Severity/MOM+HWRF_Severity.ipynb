{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from datetime import date,timedelta,datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mofunc(row):\n",
    "    if row['Severity'] > 0.8 or row['Hazard_Score'] > 80:\n",
    "        return 'Warning'\n",
    "    elif 0.6 < row['Severity'] < 0.80 or 60 < row['Hazard_Score'] < 80:\n",
    "        return 'Watch'\n",
    "    elif 0.35 < row['Severity'] < 0.6 or 35 < row['Hazard_Score'] < 60:\n",
    "        return 'Advisory'\n",
    "    else:\n",
    "        return 'Information'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcast_date = date.today()\n",
    "cur_year, cur_month,cur_day = map(str,[forcast_date.today().year,forcast_date.today().month,forcast_date.today().day])\n",
    "cur_month = cur_month.zfill(2)\n",
    "cur_day=cur_day.zfill(2)\n",
    "GFMS='Flood_byStor_'+cur_year+cur_month+cur_day+'00'+'.csv'\n",
    "GloFas='threspoints_'+cur_year+cur_month+cur_day+'00.csv'\n",
    "HWRF=\"hwrf.\"+cur_year+cur_month+cur_day+'00'+\"rainfall.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFMS=\"Flood_byStor_2021012121.csv\"\n",
    "GloFas=\"threspoints_2021012100.csv\"\n",
    "HWRF=\"eloise12s.2021012118.rainfall.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightage = read_data('weightage.csv')\n",
    "HWRF_weightage = read_data('HWRF_Weightage.csv')\n",
    "add_field_GloFas = ['Alert_Score', 'PeakArrivalScore', 'TwoYScore', 'FiveYScore', 'TwtyYScore', 'Sum_Score']\n",
    "add_field_GFMS = ['GFMS_area_score', 'GFMS_perc_area_score', 'MeanD_Score', 'MaxD_Score', 'Duration_Score', 'Sum_Score']\n",
    "add_field_HWRF=['HWRF_area_score', 'HWRF_percarea_score', 'MeanRain_Score', 'MaxRain_Score', 'HWRFTot_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read GFMS Processing data and calculte score\n",
    "with open(GFMS, 'r', encoding='UTF-8') as GFMS_file:\n",
    "    GFMS_reader = csv.reader(GFMS_file)\n",
    "    csvfile = open('GFMS_w_score.csv', 'w', newline='\\n', encoding='utf-8')\n",
    "    GFMS_w_score = csv.writer(csvfile)\n",
    "    row_count = 1\n",
    "    # csv_writer = csv.writer(write_obj)\n",
    "    for row in GFMS_reader:\n",
    "        if row_count == 1:\n",
    "            for x in add_field_GFMS:\n",
    "                row.append(x)\n",
    "            row_count = row_count + 1\n",
    "        else:\n",
    "            if float(row[1]) / float(weightage.GFMS_Area_wt) > float(weightage.GFMS_Area_max_pt):\n",
    "                GFMS_area_score = str(float(weightage.GFMS_Area_max_pt))\n",
    "            else:\n",
    "                GFMS_area_score = str(float(weightage.GFMS_Area_Min_pt) * float(row[1]) / float(weightage.GFMS_Area_wt))\n",
    "            if float(row[2]) / float(weightage.GFMS_percArea_wt) > float(weightage.GFMS_percArea_Maxpt):\n",
    "                GFMS_perc_area_score = str(float(weightage.GFMS_percArea_Maxpt))\n",
    "            else:\n",
    "                GFMS_perc_area_score = str(float(weightage.GFMS_percArea_Minpt) * float(row[2]) / float(weightage.GFMS_percArea_wt))\n",
    "            if float(row[3]) / float(weightage.GFMS_Meandepth_wt) > float(weightage.GFMS_Meandepth_Maxpt):\n",
    "                MeanD_Score = str(float(weightage.GFMS_Meandepth_Maxpt))\n",
    "            else:\n",
    "                MeanD_Score = str(float(weightage.GFMS_Meandepth_Minpt)*float(row[3]) / float(weightage.GFMS_Meandepth_wt))\n",
    "            if float(row[4]) / float(weightage.GFMS_Maxdepth_wt) > float(weightage.GFMS_Maxdepth_Maxpt):\n",
    "                MaxD_Score = str(float(weightage.GFMS_Maxdepth_Maxpt))\n",
    "            else:\n",
    "                MaxD_Score = str(float(weightage.GFMS_Maxdepth_Minpt) * float(row[4]) / float(weightage.GFMS_Maxdepth_wt))\n",
    "            if float(row[5]) / float(weightage.GFMS_Duration_wt) > float(weightage.GFMS_Duration_Maxpt):\n",
    "                Duration_Score = str(float(weightage.GFMS_Duration_Maxpt))\n",
    "            else:\n",
    "                Duration_Score = str(float(weightage.GFMS_Duration_Minpt) * float(row[5]) / float(weightage.GFMS_Duration_wt))\n",
    "            Sum_Score = str(\n",
    "                float(GFMS_area_score) + float(GFMS_perc_area_score) + float(MeanD_Score) + float(MaxD_Score) + float(\n",
    "                    Duration_Score))\n",
    "            score_field = [GFMS_area_score, GFMS_perc_area_score, MeanD_Score, MaxD_Score, Duration_Score, Sum_Score]\n",
    "            for x in score_field:\n",
    "                row.append(x)\n",
    "        GFMS_w_score.writerow(row)\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read GloFas data and Calculate score\n",
    "with open(GloFas, 'r', encoding='UTF-8') as GloFas_file:\n",
    "    GloFas_reader = csv.reader(GloFas_file)\n",
    "    csvfile = open('GloFas_w_score.csv', 'w', newline='\\n', encoding='utf-8')\n",
    "    txtfile = open('GloFas_Error.csv', 'w', newline='\\n')\n",
    "    GloFas_w_score = csv.writer(csvfile)\n",
    "    errorfile = csv.writer(txtfile)\n",
    "    row_count = 1\n",
    "    for row in GloFas_reader:\n",
    "        if row_count == 1:\n",
    "            for x in add_field_GloFas:\n",
    "                row.append(x)\n",
    "            write = [row[14], row[12], row[13], row[9], row[10], row[11], row[15], row[16], row[17], row[18], row[19],\n",
    "                     row[20]]\n",
    "            GloFas_w_score.writerow(write)\n",
    "            errorfile.writerow([row[0], row[1], row[14], 'Error'])\n",
    "            row_count = row_count + 1\n",
    "        elif float(row[12]) > 3 or float(row[12]) < 0:\n",
    "            error = \"Alert less than 0 or greater than 3 is encountered\"\n",
    "            errorfile.writerow([row[0], row[1], row[14], error])\n",
    "        elif float(row[9]) > 100:\n",
    "            error = \"2 yr EPS greater than 100 is encountered\"\n",
    "            errorfile.writerow([row[0], row[1], row[14], error])\n",
    "        elif float(row[10]) > 100:\n",
    "            error = \"5 yr EPS greater than 100 is encountered\"\n",
    "            errorfile.writerow([row[0], row[1], row[14], error])\n",
    "        elif float(row[11]) > 100:\n",
    "            error = \"20 yr EPS greater than 100 is encountered\"\n",
    "            errorfile.writerow([row[0], row[1], row[14], error])\n",
    "        elif float(row[13]) > 30:\n",
    "            error = \"Peak arrival days greater than 30 is encountered\"\n",
    "            errorfile.writerow([row[0], row[1], row[14], error])\n",
    "        else:\n",
    "            Alert_Score = str(round(float(row[12]) * float(weightage.Alert_score)))\n",
    "            TwoYScore = str(float(row[9]) / float(weightage.EPS_Twoyear_wt))\n",
    "            FiveYScore = str(float(row[10]) / float(weightage.EPS_Fiveyear_wt))\n",
    "            TwtyYScore = str(float(row[11]) / float(weightage.EPS_Twtyyear_wt))\n",
    "            if int(row[9]) == 0 and int(row[10]) == 0 and int(row[11]) == 0 and int(row[12]) == 0:\n",
    "                PeakArrival_Score = str(0)\n",
    "            elif int(row[13]) == 10 or int(row[13]) > 10:\n",
    "                PeakArrival_Score = str(1)\n",
    "            elif int(row[13]) == 9:\n",
    "                PeakArrival_Score = str(2)\n",
    "            elif int(row[13]) == 8:\n",
    "                PeakArrival_Score = str(3)\n",
    "            elif int(row[13]) == 7:\n",
    "                PeakArrival_Score = str(4)\n",
    "            elif int(row[13]) == 6:\n",
    "                PeakArrival_Score = str(5)\n",
    "            elif int(row[13]) == 5:\n",
    "                PeakArrival_Score = str(6)\n",
    "            elif int(row[13]) == 4:\n",
    "                PeakArrival_Score = str(7)\n",
    "            elif int(row[13]) == 3:\n",
    "                PeakArrival_Score = str(8)\n",
    "            elif int(row[13]) == 2:\n",
    "                PeakArrival_Score = str(9)\n",
    "            elif int(row[13]) == 1:\n",
    "                PeakArrival_Score = str(10)\n",
    "            Sum_Score = str(\n",
    "                float(Alert_Score) + float(PeakArrival_Score) + float(TwoYScore) + float(FiveYScore) + float(\n",
    "                    TwtyYScore))\n",
    "            score_field = [Alert_Score, PeakArrival_Score, TwoYScore, FiveYScore, TwtyYScore, Sum_Score]\n",
    "            for x in score_field:\n",
    "                row.append(x)\n",
    "            write = [row[14], row[12], row[13], row[9], row[10], row[11], row[15], row[16], row[17], row[18], row[19],\n",
    "                 row[20]]\n",
    "            GloFas_w_score.writerow(write)\n",
    "csvfile.close()\n",
    "txtfile.close()\n",
    "GloFas = read_data('GloFas_w_score.csv')\n",
    "GloFas.sort_values(by='pfaf_id', ascending=True, inplace=True)\n",
    "GloFas.set_index('pfaf_id').to_csv('GloFas_w_score_sort.csv', encoding='utf-8')\n",
    "\n",
    "with open('GloFas_w_score_sort.csv', 'r') as GloFas_file:\n",
    "    GloFas_reader = csv.reader(GloFas_file)\n",
    "    csvfile = open('GloFas_w_Avgscore.csv', 'w', newline='\\n', encoding='utf-8')\n",
    "    GloFas_w_score = csv.writer(csvfile)\n",
    "    Haz_Score = 0\n",
    "    pfaf_id = -1\n",
    "    similarity = 0\n",
    "    i = 1\n",
    "    To_be_written = 'False'\n",
    "    write = []\n",
    "    for row in GloFas_reader:\n",
    "        if i == 1:\n",
    "            i = i + 1\n",
    "            GloFas_w_score.writerow(row)\n",
    "        else:\n",
    "            if pfaf_id == -1 or pfaf_id == row[0]:\n",
    "                pfaf_id = row[0]\n",
    "                Haz_Score = Haz_Score + float(row[11])\n",
    "                similarity = similarity + 1\n",
    "                last_row = row\n",
    "                To_be_written = 'True'\n",
    "            else:\n",
    "                last_row[11] = str(Haz_Score / similarity)\n",
    "                GloFas_w_score.writerow(last_row)\n",
    "                last_row = row\n",
    "                pfaf_id = row[0]\n",
    "                Haz_Score = float(row[11])\n",
    "                similarity = 1\n",
    "                To_be_written = 'True'\n",
    "if To_be_written == 'True':\n",
    "    last_row[11] = str(Haz_Score / similarity)\n",
    "    GloFas_w_score.writerow(last_row)\n",
    "csvfile.close()\n",
    "# Glofas Done\n",
    "os.remove('GloFas_w_score_sort.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read HWRF rainfall processed data and calculate separate hazard Score\n",
    "try:\n"
    " with open(HWRF, 'r', encoding='UTF-8') as HWRF_file:\n",
    "     HWRF_reader = csv.reader(HWRF_file)\n",
    "     csvfile = open('HWRF_w_score.csv', 'w', newline='\\n', encoding='utf-8')\n",
    "     HWRF_w_score = csv.writer(csvfile)\n",
    "     row_count = 1\n",
    "     # csv_writer = csv.writer(write_obj)\n",
    "     for row in HWRF_reader:\n",
    "         if row_count == 1:\n",
    "             for x in add_field_HWRF:\n",
    "                 row.append(x)\n",
    "             HWRF_w_score.writerow(row)\n",
    "             row_count = row_count + 1\n",
    "         else:\n",
    "             if float(row[1]) / float(HWRF_weightage.HWRF_Area_wt) > float(HWRF_weightage.HWRF_Area_max_pt):\n",
    "                 HWRF_area_score = str(float(HWRF_weightage.HWRF_Area_max_pt))\n",
    "             else:\n",
    "                 HWRF_area_score = str(float(HWRF_weightage.HWRF_Area_Min_pt) * float(row[1]) / float(HWRF_weightage.HWRF_Area_wt))\n",
    "             if float(row[2]) / float(HWRF_weightage.HWRF_percArea_wt) > float(HWRF_weightage.HWRF_percArea_Maxpt):\n",
    "                 HWRF_percarea_score = str(float(HWRF_weightage.HWRF_percArea_Maxpt))\n",
    "             else:\n",
    "                 HWRF_percarea_score = str(float(HWRF_weightage.HWRF_percArea_Minpt) * float(row[2]) / float(HWRF_weightage.HWRF_percArea_wt))\n",
    "             if float(row[3]) >= float(HWRF_weightage.HWRF_MeanRain_minwt):\n",
    "                 if ((float(row[3])- float(HWRF_weightage.HWRF_MeanRain_minwt))/ float(HWRF_weightage.HWRF_MeanRain_increment))+float(HWRF_weightage.HWRF_MeanRain_Minpt) > float(HWRF_weightage.HWRF_MeanRain_Maxpt):\n",
    "                     MeanRain_core = str(float(HWRF_weightage.HWRF_MeanRain_Maxpt))\n",
    "                 else:\n",
    "                     MeanRain_Score = str(((float(row[3])- float(HWRF_weightage.HWRF_MeanRain_minwt))/ float(HWRF_weightage.HWRF_MeanRain_increment))+float(HWRF_weightage.HWRF_MeanRain_Minpt))\n",
    "             else:\n",
    "                  MeanRain_Score='0'\n",
    "             if float(row[4]) >= float(HWRF_weightage.HWRF_MaxRain_minwt):                       \n",
    "                 if ((float(row[4])- float(HWRF_weightage.HWRF_MaxRain_minwt))/ float(HWRF_weightage.HWRF_MaxRain_increment))+float(HWRF_weightage.HWRF_MaxRain_Minpt) > float(HWRF_weightage.HWRF_MaxRain_Maxpt):\n",
    "                     MaxRain_Score = str(float(HWRF_weightage.HWRF_MaxRain_Maxpt))\n",
    "                 else:\n",
    "                     MaxRain_Score = str(((float(row[4])- float(HWRF_weightage.HWRF_MaxRain_minwt))/ float(HWRF_weightage.HWRF_MaxRain_increment))+float(HWRF_weightage.HWRF_MaxRain_Minpt))\n",
    "             else:\n",
    "                 MaxRain_Score='0'\n",
    "             HWRFTot_Score = (float(HWRF_area_score)+float(HWRF_percarea_score)+ float(MeanRain_Score)+float(MaxRain_Score))*2.5 \n",
    "             results_list = [row[0], row[1], row[2], row[3], row[4],HWRF_area_score,HWRF_percarea_score,MeanRain_Score,MaxRain_Score, HWRFTot_Score]\n",
    "             HWRF_w_score.writerow(results_list)"
    "except:\n"
    " pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-d14011890510>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Final_Attributes['Sum_Score_x'][(Final_Attributes['Sum_Score_y'] == 0)] = Final_Attributes['Sum_Score_x']*2\n",
      "<ipython-input-12-d14011890510>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Final_Attributes['Sum_Score_y'][(Final_Attributes['Sum_Score_x'] == 0)] = Final_Attributes['Sum_Score_y']*2\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1847: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  cond2 = (x >= np.asarray(_b)) & cond0\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'HWRF_w_score.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d14011890510>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GloFas_w_Avgscore.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GFMS_w_score.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HWRF_w_score.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'HWRF_w_score.csv'"
     ]
    }
   ],
   "source": [
    "GFMS = read_data('GFMS_w_score.csv')\n",
    "GloFas = read_data('GloFas_w_Avgscore.csv')\n",
    "Attributes = read_data('Attributes.csv')\n",
    "#HWRF=read_data('HWRF_w_score.csv')\n",
    "join = pd.merge(GloFas.set_index('pfaf_id'), GFMS.set_index('pfaf_id'), on='pfaf_id', how='outer')\n",
    "#join0=pd.merge(join, HWRF.set_index('pfaf_id'), on='pfaf_id', how='outer')\n",
    "PDC_resilience = read_data('Copy of Resilience_Index.csv')\n",
    "join1 = pd.merge(Attributes, PDC_resilience[['ISO', 'Resilience_Index', ' NormalizedLackofResilience ']], on='ISO', how='inner')\n",
    "#Final_Attributes = pd.merge(join1.set_index('pfaf_id'), join0, on='pfaf_id', how='outer')\n",
    "Final_Attributes[['Sum_Score_x', 'Sum_Score_y']] = Final_Attributes[['Sum_Score_x', 'Sum_Score_y']].fillna(value=0)\n",
    "Final_Attributes['Sum_Score_x'][(Final_Attributes['Sum_Score_y'] == 0)] = Final_Attributes['Sum_Score_x']*2\n",
    "Final_Attributes['Sum_Score_y'][(Final_Attributes['Sum_Score_x'] == 0)] = Final_Attributes['Sum_Score_y']*2\n",
    "Final_Attributes = Final_Attributes.assign(\n",
    "    Hazard_Score=lambda x: Final_Attributes['Sum_Score_x'] + Final_Attributes['Sum_Score_y'])\n",
    "try:\n"
    " HWRF=read_data('HWRF_w_score.csv')\n",
    " join0=pd.merge(join, HWRF.set_index('pfaf_id'), on='pfaf_id', how='outer')\n",
    " Final_Attributes['Flag']=np.where((Final_Attributes['Hazard_Score']!=0) & (Final_Attributes['Hazard_Score']<Final_Attributes['HWRFTot_Score']),1,0)\n",
    " Final_Attributes['Hazard_Score'] =Final_Attributes[['Hazard_Score', 'HWRFTot_Score']].max(axis=1)\n",
    "except:\n"
    " pass\n"
    " Final_Attributes = Final_Attributes[Final_Attributes.Hazard_Score != 0]\n",
    " Final_Attributes = Final_Attributes.assign(\n",
    "    Scaled_Riverine_Risk=lambda x: Final_Attributes['rfr_score'] * 20)\n",
    "Final_Attributes = Final_Attributes.assign(\n",
    "    Scaled_Coastal_Risk=lambda x: Final_Attributes['cfr_score'] * 20)\n",
    "Final_Attributes = Final_Attributes.assign(\n",
    "    Severity=lambda x: scipy.stats.norm(np.log(100 - Final_Attributes[['Scaled_Riverine_Risk', 'Scaled_Coastal_Risk']].max(axis=1)), 1).cdf(\n",
    "        np.log(Final_Attributes['Hazard_Score'])))\n",
    "Final_Attributes['Alert'] = Final_Attributes.apply(mofunc, axis=1)\n",
    "Final_Attributes.to_csv('Final_Attributes'+cur_year+cur_month+cur_day+'00'+'.csv', encoding='utf-8-sig')\n",
    "Attributes_Clean = pd.merge(join1.set_index('pfaf_id'), Final_Attributes[['Alert']], on='pfaf_id', how='right')\n",
    "Attributes_Clean.to_csv('Attributes_Clean'+cur_year+cur_month+cur_day+'00'+'.csv', encoding='utf-8-sig')\n",
    "os.remove('GloFas_w_score.csv')\n",
    "os.remove('GloFas_w_Avgscore.csv')\n",
    "os.remove('GFMS_w_score.csv')\n",
    "try:\n"
    " os.remove('HWRF_w_score.csv')\n"
    "except:\n"
    " pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
